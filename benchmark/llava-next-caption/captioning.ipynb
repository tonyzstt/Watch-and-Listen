{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/data/anaconda3/envs/llava-demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import PIL\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(root_dir_data: str, split: str, sample: int = None):\n",
    "    '''\n",
    "    Get the dataset from the given root directory.\n",
    "    \n",
    "    Args:\n",
    "        root_dir_data (str): Root directory of the dataset.\n",
    "        split (str): Split of the dataset.\n",
    "        sample (int): Number of frames to sample from each video.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Any]]: Dataset.\n",
    "    '''\n",
    "    \n",
    "    def get_captions_meta(root_dir_data, split, id):\n",
    "        '''\n",
    "        Get captions from the metadata file.\n",
    "        \n",
    "        Args:\n",
    "            root_dir_data (str): Root directory of the dataset.\n",
    "            split (str): Split of the dataset.\n",
    "            id (str): Video id.\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, str]: Dictionary of captions.\n",
    "        '''\n",
    "        \n",
    "        meta_file = os.path.join(root_dir_data, split, 'metas', f'{id}.json')\n",
    "        if not os.path.exists(meta_file):\n",
    "            return None\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "            captions = {\n",
    "                'frame_caption': meta['original_metadata']['frame_caption'],\n",
    "                'music_caption': meta['original_metadata']['music_caption'],\n",
    "                'caption': meta['original_metadata']['caption'],\n",
    "                'polish_caption': meta['original_metadata']['polish_caption']\n",
    "            }\n",
    "            \n",
    "            print(\"Successfully loaded captions, id: \", id)\n",
    "            \n",
    "            return captions\n",
    "    \n",
    "    \n",
    "    def load_frames(root_dir_data, split, id, sample=None):\n",
    "        \n",
    "        '''\n",
    "        Load all frames of a video.\n",
    "        \n",
    "        Args:\n",
    "            root_dir_data (str): Root directory of the dataset.\n",
    "            split (str): Split of the dataset.\n",
    "            id (str): Video id.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: np array of frames of shape (num_frames, height, width, 3).\n",
    "        '''\n",
    "        frames = []\n",
    "        images_dir = os.path.join(root_dir_data, split, 'images', id)\n",
    "        if not os.path.exists(images_dir):\n",
    "            return None\n",
    "        for image_file in os.listdir(images_dir):\n",
    "            # read jpg image\n",
    "            image = PIL.Image.open(os.path.join(images_dir, image_file))\n",
    "            frames.append(np.array(image))\n",
    "            \n",
    "        if len(frames) == 0:\n",
    "            return None\n",
    "        \n",
    "        frames = np.stack(frames)\n",
    "        \n",
    "        if sample is not None:\n",
    "            # sample \"sample\" frames from the video\n",
    "            indices = np.linspace(0, frames.shape[0] - 1, sample).astype(int)\n",
    "            frames = frames[indices]\n",
    "            \n",
    "        # put to device\n",
    "        frames = torch.tensor(frames).to(device)\n",
    "        \n",
    "        print(\"Successfully loaded frames, id: \", id)\n",
    "        return frames\n",
    "    \n",
    "    \n",
    "    # Get all video ids from test.json\n",
    "    id_file = os.path.join(root_dir_data, f'{split}.json')\n",
    "    assert os.path.exists(id_file), f'{id_file} does not exist'\n",
    "    video_ids = []\n",
    "    with open(id_file, 'r') as f:\n",
    "        video_ids = json.load(f)\n",
    "    \n",
    "    dataset = {}\n",
    "    print(f'Loading {len(video_ids)} videos')\n",
    "    for id in tqdm.tqdm(video_ids):\n",
    "        images = load_frames(root_dir_data, split, id, sample=sample)\n",
    "        if images is None:\n",
    "            continue\n",
    "        \n",
    "        captions = get_captions_meta(root_dir_data, split, id)\n",
    "        # TODO: video and audio\n",
    "        if captions is not None:\n",
    "            dataset[id] = {\n",
    "                'captions': captions,\n",
    "                'images': images\n",
    "            }\n",
    "            \n",
    "    print(f'Loaded {len(dataset)} videos')\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llava_next_model(model_path: str):\n",
    "    '''\n",
    "    Load the model from the given path.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the model.\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Model.\n",
    "    '''\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    processor = LlavaNextVideoProcessor.from_pretrained(model_path)\n",
    "    model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_video(model, processor, dataset):\n",
    "    '''\n",
    "    Generate captions for videos in the given dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        processor (LlavaNextVideoProcessor): Processor.\n",
    "        dataset (Dict[str, Dict[str, Any]]): Dataset.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of captions.\n",
    "    '''\n",
    "    \n",
    "    conversation_video = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe the video.\"},\n",
    "                    {\"type\": \"video\"},\n",
    "                ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt = processor.apply_chat_template(conversation_video, add_generation_prompt=True)\n",
    "    print(prompt)\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    for id in dataset:\n",
    "        inputs = processor([prompt], videos=[dataset[id]['images']], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "        generate_kwargs = {\"max_new_tokens\": 200, \"do_sample\": True, \"top_p\": 0.9}\n",
    "        output = model.generate(**inputs, **generate_kwargs)\n",
    "        generated_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "        # get only output from the assistant\n",
    "        generated_text = generated_text.split('ASSISTANT: ')[-1]\n",
    "        print(generated_text)\n",
    "        outputs[id] = generated_text\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 80 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:03<01:14,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded frames, id:  0erDutDPHxc\n",
      "Successfully loaded captions, id:  0erDutDPHxc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/80 [00:07<00:15,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded frames, id:  -2x2NMwBDzE\n",
      "Successfully loaded captions, id:  -2x2NMwBDzE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 37/80 [00:11<00:13,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded frames, id:  _mCzE_THQaQ\n",
      "Successfully loaded captions, id:  _mCzE_THQaQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 49/80 [00:15<00:09,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded frames, id:  1tqn9d_w3qo\n",
      "Successfully loaded captions, id:  1tqn9d_w3qo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 51/80 [00:35<00:31,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded frames, id:  1eTqgXxDfvA\n",
      "Successfully loaded captions, id:  1eTqgXxDfvA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:00<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded frames, id:  0CkL_9X3rS8\n",
      "Successfully loaded captions, id:  0CkL_9X3rS8\n",
      "Loaded 6 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_dir_data = '/home/saberwu2002/disk-data/data/MMTrail_processed'\n",
    "split = 'test'\n",
    "sample = 30\n",
    "dataset = get_dataset(root_dir_data, split, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/saberwu2002/disk-data/checkpoints/llava-next-video-7b-hf'\n",
    "model, processor = load_llava_next_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: <video>\n",
      "Describe the video. ASSISTANT:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video shows a man standing in front of a band, wearing a suit and a blue shirt, holding a baton which is pointed towards the musicians. The man is using a pair of binoculars, presumably to watch a performer who is not visible in the shot. The setting appears to be a band performance, with the audience watching the man on stage while the performer continues with the music, probably playing an instrument that's not visible in the shot.\n",
      "The video shows a woman with long hair and adorned with multiple necklaces and bangles, wearing a blue sari with a gold border, standing in front of a brightly lit background with vibrant colors. She appears to be an actress in a South Asian context, perhaps from a television show given her attire and the environment. She is engaged in a serious conversation with someone off-camera, her expression indicating concern or a thoughtful demeanor, as she listens intently. Another person, not in full view, is seated or standing behind her, contributing to the atmosphere. The lighting is vivid, with a combination of ambient and focused lighting highlighting her face, enhancing the dramatic effect of the scene.\n",
      "The video shows a close-up view of a green animal, likely a cow or a similar large mammal, with visible muscles and skin. It is standing in front of a counter that appears to be in a kitchen setting, filled with various cooked meats and cuts of meat displayed. There is a person visible at the far end of the counter, who seems to be preparing food in the background. The counter is well-lit, with items like a bottle and other food preparation ingredients clearly visible. The animal looks at the camera and then towards the person at the end of the counter.\n",
      "The video shows two couples dressed in formal attire, likely at a wedding or a similar event. One woman is dressed in a sophisticated white dress, while the other appears to be wearing a classic black suit with a tie, suggesting a formal or traditional style. Both couples are engaged in a gesture of holding hands, possibly during a moment of unity or a dance. The woman in the black suit is speaking to a man and appears to be the one initiating this action, leading by taking her hand. The setting looks like a room or hall with classical decor, and there is an audience in the background watching or waiting. The overall atmosphere seems polite and formal.\n",
      "The video shows a person sitting down with their head partially covered by their hand. The person is wearing a bright blue, one-piece costume with a design that somewhat resembles the face of Cookie Monster from the children's television show \"Sesame Street.\" They are also wearing black gloves and are seated on a chair that appears to be inside a home. There are two tiled walls visible in the background, suggesting a domestic setting. The lighting in the video is quite dim, and the person's face is not fully visible due to their hand and the angle of the camera. The costume itself is detailed with a design that features the eyes, mouth, and possibly additional facial features, typical of the character. The person is seated quietly, and their actions are not clear from this still shot. The scene conveys a sense of playful or humorous attire.\n",
      "The video captures the process of preparing a meal in a kitchen, specifically, it's focused on frying something that resembles a large piece of chicken thigh. In the video, we see a person, most likely the person cooking, standing in front of a counter with various cooking implements and ingredients laid out on the counter. They are using a large spatula to flip and turn the piece of meat, which appears to be coated in breadcrumbs, ensuring that it is cooked evenly on all sides. The person is wearing an apron, indicating a casual and homely environment. The person appears to be using a large pan, which might be a wok or a deep frying pan, as the food is browning nicely, suggesting that the heat is at the right level for a perfect fry. We see a wooden spoon in the background, and an orange and a\n"
     ]
    }
   ],
   "source": [
    "captions = generate_caption_video(model, processor, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "with open('captions.json', 'w') as f:\n",
    "    json.dump(captions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(reference_summary, candidate_summary):\n",
    "\n",
    "    reference_tokens = nltk.word_tokenize(reference_summary.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_summary.lower())\n",
    "\n",
    "    references = [reference_tokens]\n",
    "    candidate = candidate_tokens\n",
    "\n",
    "    chencherry = SmoothingFunction()\n",
    "\n",
    "    bleu_score = sentence_bleu(\n",
    "        references,\n",
    "        candidate,\n",
    "        smoothing_function=chencherry.method1\n",
    "    )\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "def compute_similarity(reference_summary, candidate_summary):\n",
    "    model = SentenceTransformer('/home/saberwu2002/disk-data/checkpoints/sentence-transformers_all-MiniLM-L6-v2')\n",
    "\n",
    "    embedding1 = model.encode(reference_summary, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(candidate_summary, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(embedding1, embedding2)\n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_for_video(metadata_folder, video_filename):\n",
    "    \"\"\"Loads metadata for a specific video from its corresponding JSON file.\"\"\"\n",
    "    \n",
    "    metadata_file = os.path.join(metadata_folder, f\"{os.path.splitext(video_filename)[0]}.json\")\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, 'r') as file:\n",
    "            metadata = json.load(file)\n",
    "        return metadata[\"original_metadata\"][\"caption\"]  # Extract the reference title\n",
    "    return None\n",
    "\n",
    "def load_generated_titles(output_json):\n",
    "    \"\"\"Loads generated titles from the output file.\"\"\"\n",
    "    generated_titles = {}\n",
    "    captions = json.load(open(output_json))\n",
    "    for video_filename, caption in captions.items():\n",
    "        generated_titles[video_filename] = caption\n",
    "    return generated_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_titles(metadata_file, output_file):\n",
    "    \"\"\"Compares generated titles with reference titles and computes BLEU and similarity scores.\"\"\"\n",
    "    generated_titles = load_generated_titles(output_file)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for video_filename, generated_title in generated_titles.items():\n",
    "        reference_title = load_metadata_for_video(metadata_folder, video_filename)\n",
    "\n",
    "        if reference_title:\n",
    "            bleu_score = compute_bleu_score(reference_title, generated_title)\n",
    "            similarity_score = compute_similarity(reference_title, generated_title)\n",
    "\n",
    "            results[video_filename] = {\n",
    "                \"reference_title\": reference_title,\n",
    "                \"generated_title\": generated_title,\n",
    "                \"bleu_score\": bleu_score,\n",
    "                \"similarity_score\": similarity_score\n",
    "            }\n",
    "        else:\n",
    "            results[video_filename] = {\n",
    "                \"reference_title\": None,\n",
    "                \"generated_title\": generated_title,\n",
    "                \"bleu_score\": None,\n",
    "                \"similarity_score\": None,\n",
    "                \"error\": \"Metadata file not found\"\n",
    "            }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, output_json):\n",
    "    \"\"\"Saves evaluation results to a JSON file.\"\"\"\n",
    "    with open(output_json, 'w') as file:\n",
    "        json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed. Results saved to results.json\n"
     ]
    }
   ],
   "source": [
    "metadata_folder = \"/home/saberwu2002/disk-data/data/MMTrail_processed/test/metas\"\n",
    "output_file = \"/home/saberwu2002/CS229-Project/benchmark/llava-next-caption/captions.json\"\n",
    "output_json = \"results.json\" \n",
    "\n",
    "evaluation_results = evaluate_titles(metadata_folder, output_file)\n",
    "save_results(evaluation_results, output_json)\n",
    "\n",
    "print(f\"Evaluation completed. Results saved to {output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
