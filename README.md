# Watch and Listen: Personalized Video Summarization with Multi-Modal Learning

### Abstract

Video summarization is key to managing the ever-growing volume of video content. Personalized summarization, which tailors highlights to individual preferences, offers a more engaging and relevant viewing experience. However, understanding user preferences from video content requires leveraging multiple modalities, including visual, auditory, and textual information. Traditional approaches often fail to integrate these modalities effectively, limiting their ability to create accurate and personalized summaries. In this project, we introduce a multi-modal learning framework that combines information from both video and audio sources to create personalized video summaries. Evaluation results on benchmark datasets demonstrate that our approach achieves competitive performance relative to state-of-the-art models. Overall, our framework advances video summarization by incorporating diverse modalities to produce contextually rich and personalized summaries, offering promising directions for future research in dynamic multimedia environments.

### Methods
<object data="assets/pipeline.pdf" type="application/pdf" width="100%" height="600px">
  <p>Your browser does not support PDFs.
     <a href="assets/pipeline.pdf">Download the PDF</a>.
  </p>
</object>
